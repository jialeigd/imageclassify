{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将数据转换为tf_Records\n",
    "**转换的意义主要为方便数据的存储和读写操作,同时也可以加快训练速度**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random  # 为了后面随机化数据\n",
    "import tensorflow as tf\n",
    "import cv2 # 为了读取图像\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "import scipy.io as sio  \n",
    "import cv2\n",
    "\n",
    "class vgg16:\n",
    "    def __init__(self, imgs, weights=None, sess=None):\n",
    "        self.imgs = imgs\n",
    "        self.convlayers()\n",
    "        self.fc_layers()\n",
    "        self.probs = tf.nn.softmax(self.fc3l)\n",
    "        if weights is not None and sess is not None:\n",
    "            self.load_weights(weights, sess)\n",
    "\n",
    "\n",
    "    def convlayers(self):\n",
    "        self.parameters = []\n",
    "\n",
    "        # zero-mean input\n",
    "        with tf.name_scope('preprocess') as scope:\n",
    "            mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')\n",
    "            images = self.imgs-mean\n",
    "\n",
    "        # conv1_1\n",
    "        with tf.name_scope('conv1_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv1_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv1_2\n",
    "        with tf.name_scope('conv1_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv1_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool1\n",
    "        self.pool1 = tf.nn.max_pool(self.conv1_2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool1')\n",
    "\n",
    "        # conv2_1\n",
    "        with tf.name_scope('conv2_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv2_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv2_2\n",
    "        with tf.name_scope('conv2_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv2_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool2\n",
    "        self.pool2 = tf.nn.max_pool(self.conv2_2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool2')\n",
    "\n",
    "        # conv3_1\n",
    "        with tf.name_scope('conv3_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv3_2\n",
    "        with tf.name_scope('conv3_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv3_3\n",
    "        with tf.name_scope('conv3_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool3\n",
    "        self.pool3 = tf.nn.max_pool(self.conv3_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool3')\n",
    "\n",
    "        # conv4_1\n",
    "        with tf.name_scope('conv4_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv4_2\n",
    "        with tf.name_scope('conv4_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv4_3\n",
    "        with tf.name_scope('conv4_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool4\n",
    "        self.pool4 = tf.nn.max_pool(self.conv4_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool4')\n",
    "\n",
    "        # conv5_1\n",
    "        with tf.name_scope('conv5_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool4, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv5_2\n",
    "        with tf.name_scope('conv5_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv5_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv5_3\n",
    "        with tf.name_scope('conv5_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv5_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool5\n",
    "        self.pool5 = tf.nn.max_pool(self.conv5_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool4')\n",
    "\n",
    "    def fc_layers(self):\n",
    "        # fc1\n",
    "        with tf.name_scope('fc1') as scope:\n",
    "            shape = int(np.prod(self.pool5.get_shape()[1:]))\n",
    "            fc1w = tf.Variable(tf.truncated_normal([shape, 4096],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            pool5_flat = tf.reshape(self.pool5, [-1, shape])\n",
    "            fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)\n",
    "            self.fc1 = tf.nn.relu(fc1l)\n",
    "            self.parameters += [fc1w, fc1b]\n",
    "\n",
    "        # fc2\n",
    "        with tf.name_scope('fc2') as scope:\n",
    "            fc2w = tf.Variable(tf.truncated_normal([4096, 4096],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc2b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            fc2l = tf.nn.bias_add(tf.matmul(self.fc1, fc2w), fc2b)\n",
    "            self.fc2 = tf.nn.relu(fc2l)\n",
    "            self.parameters += [fc2w, fc2b]\n",
    "\n",
    "        # fc3\n",
    "        with tf.name_scope('fc3') as scope:\n",
    "            fc3w = tf.Variable(tf.truncated_normal([4096, 1000],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc3b = tf.Variable(tf.constant(1.0, shape=[1000], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            self.fc3l = tf.nn.bias_add(tf.matmul(self.fc2, fc3w), fc3b)\n",
    "            self.parameters += [fc3w, fc3b]\n",
    "\n",
    "    def load_weights(self, weight_file, sess):\n",
    "        weights = np.load(weight_file)\n",
    "        keys = sorted(weights.keys())\n",
    "        for i, k in enumerate(keys):\n",
    "            print i, k, np.shape(weights[k])\n",
    "            sess.run(self.parameters[i].assign(weights[k]))\n",
    "\n",
    "\n",
    "def save_feature(feature_normal,file_path):\n",
    "\t\n",
    "\timport os\n",
    "\tif not os.path.exists(file_path):\n",
    "\t\tos.mkdir(file_path)\n",
    "\n",
    "\tshape=feature_normal.shape\n",
    "\tfeature = np.reshape(feature_normal,[shape[1],shape[2],shape[3]])\n",
    "\tfinal_feature=feature[:,:,0]\n",
    "\n",
    "\tconnect_Feature=np.zeros([shape[1]*shape[3],shape[2]])\n",
    "\n",
    "\tfor i in range(shape[3]):\n",
    "\t\tfinal_feature+=feature[:,:,i]\n",
    "\t\tconnect_Feature[i*shape[2]:(i+1)*shape[2],:]=feature[:,:,i]\n",
    "\t\t\n",
    "\tcv2.imwrite(file_path+\"/\"+\"final_feature.jpg\",final_feature/shape[3])\n",
    "\n",
    "\tcv2.imwrite(file_path+\"/\"+\"connect_Feature.jpg\",connect_Feature)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    sess = tf.Session()  \n",
    "\n",
    "    img_name=\"test.jpg\"\n",
    "    model_name=\"vgg16_weights.npz\"\n",
    "\n",
    "    imgs = tf.placeholder(tf.float32, [None, 224, 224, 3])  \n",
    "\n",
    "    vgg = vgg16(imgs, model_name, sess)  \n",
    "      \n",
    "    img= imread(img_name, mode='RGB')\n",
    "\n",
    "    img= imresize(img, (224, 224))   \n",
    "      \n",
    "    #img=np.ones([224,224,3])\n",
    "\n",
    "\n",
    "\n",
    "    feature_fc3= sess.run(vgg.fc3l, feed_dict={vgg.imgs:[img]})\n",
    "\n",
    "    print(\"feature_fc3:\",feature_fc3.shape,feature_fc3)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    feature_conv1_1,feature_conv1_2,feature_conv2_1,feature_conv2_2,feature_conv3_1,feature_conv3_2,feature_conv3_3,feature_conv4_1,feature_conv4_2,feature_conv4_3,feature_conv5_1,feature_conv5_2,feature_conv5_3= sess.run((vgg.conv1_1,vgg.conv1_2,vgg.conv2_1,vgg.conv2_2,vgg.conv3_1,vgg.conv3_2,vgg.conv3_3,vgg.conv4_1,vgg.conv4_2,vgg.conv4_3,vgg.conv5_1,vgg.conv5_2,vgg.conv5_3), feed_dict={vgg.imgs: [img]}) \n",
    "\n",
    "\n",
    "    save_feature(feature_conv1_1,\"feature_conv1_1\")\n",
    "    save_feature(feature_conv1_2,\"feature_conv1_2\")\n",
    "    save_feature(feature_conv2_1,\"feature_conv2_1\")\n",
    "    save_feature(feature_conv2_2,\"feature_conv2_2\")\n",
    "    save_feature(feature_conv3_1,\"feature_conv3_1\")\n",
    "    save_feature(feature_conv3_2,\"feature_conv3_2\")\n",
    "    save_feature(feature_conv3_3,\"feature_conv3_3\")\n",
    "    save_feature(feature_conv4_1,\"feature_conv4_1\")\n",
    "    save_feature(feature_conv4_2,\"feature_conv4_2\")\n",
    "    save_feature(feature_conv4_3,\"feature_conv4_3\")\n",
    "    save_feature(feature_conv5_1,\"feature_conv5_1\")\n",
    "    save_feature(feature_conv5_2,\"feature_conv5_2\")\n",
    "    save_feature(feature_conv5_3,\"feature_conv5_3\")\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义相关预处理函数，可完成数据转换工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "def _process_image_withoutcoder(filename):\n",
    "    image = cv2.imread(filename)\n",
    "    if not image is None:\n",
    "        image_data = image.tostring()\n",
    "        assert len(image.shape) == 3\n",
    "        height = image.shape[0]\n",
    "        width = image.shape[1]\n",
    "        assert image.shape[2] == 3\n",
    "        return image_data, height, width\n",
    "    else:\n",
    "        return None,None,None\n",
    "\n",
    "\n",
    "def _convert_to_example_simple(image_example, image_buffer):\n",
    "    \"\"\"\n",
    "    image_example\n",
    "    image_buffer\n",
    "    \"\"\"\n",
    "    class_label = image_example['label']\n",
    "    \n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/encoded': _bytes_feature(image_buffer),\n",
    "        'image/label': _int64_feature(class_label),\n",
    "    }))\n",
    "    return example\n",
    "\n",
    "\n",
    "def _add_to_tfrecord(filename, image_example, tfrecord_writer):\n",
    "    image_data, _, _ = _process_image_withoutcoder(filename)\n",
    "    if not image_data is None:\n",
    "        example = _convert_to_example_simple(image_example, image_data)\n",
    "        tfrecord_writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_tf_records(data_dir,data_annotation_name):\n",
    "    \"\"\"\n",
    "    用于产生生产tf_records\n",
    "    \n",
    "        data_dir\n",
    "            -label_images1\n",
    "            -label_images2\n",
    "            \n",
    "        train_data.txt   \n",
    "            image_path label\n",
    "            image_path label\n",
    "        \n",
    "    \"\"\"\n",
    "    print(\"start create tf records\")\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "    if not os.path.exists(data_annotation_name):\n",
    "        print(\"data_annotation_name  is not exists\")\n",
    "     \n",
    "    \n",
    "    # 设置输入的tf_Records 路径\n",
    "    file_output=os.path.join(data_dir,\"train_data.tfrecord_shuffle\")\n",
    "    \n",
    "    f_lines=open(data_annotation_name).readlines()\n",
    "    dataset = []\n",
    "    for line_index,f_line in enumerate(f_lines):\n",
    "        info = f_line.strip().split()\n",
    "        data_example = dict()\n",
    "        bbox = dict()\n",
    "        \n",
    "        data_example[\"file_name\"] = info[0] #路径\n",
    "        \n",
    "        data_example[\"label\"] = int(info[1]) #label\n",
    "        \n",
    "        dataset.append(data_example)\n",
    "        \n",
    "        if (line_index+1)%5000==0:\n",
    "            print(\"now process image num is : %d\"%(line_index+1))\n",
    "    \n",
    "    # 数据扰动\n",
    "    random.shuffle(dataset)\n",
    "    \n",
    "    with tf.python_io.TFRecordWriter(file_output) as tfrecord_writer:\n",
    "        for index, image_example in enumerate(dataset):\n",
    "            \n",
    "            filename = os.path.join(data_dir,image_example['file_name'])\n",
    "            \n",
    "            _add_to_tfrecord(filename, image_example, tfrecord_writer)\n",
    "            \n",
    "    tfrecord_writer.close()\n",
    "    \n",
    "    print(\"finish write tf records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start create tf records\n",
      "now process image num is : 5000\n",
      "now process image num is : 10000\n",
      "now process image num is : 15000\n",
      "now process image num is : 20000\n",
      "now process image num is : 25000\n",
      "now process image num is : 30000\n",
      "now process image num is : 35000\n",
      "now process image num is : 40000\n",
      "now process image num is : 45000\n",
      "now process image num is : 50000\n",
      "now process image num is : 55000\n",
      "finish write tf records\n"
     ]
    }
   ],
   "source": [
    "data_dir=\"dataset\"\n",
    "\n",
    "data_annotation_name=\"dataset/train_data.txt\"\n",
    "\n",
    "create_tf_records(data_dir,data_annotation_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  用于tf records数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_single_tfrecord(tfrecord_file, batch_size, image_size):\n",
    "    \"\"\"\n",
    "    tfrecord_File:路径\n",
    "    batch_size:我们一次从tfrecord中读取的数据量的大小\n",
    "    image_size:这个一般就默认，我们存入的时候图像的大小是多大就多大\n",
    "    \n",
    "    width=height\n",
    "    cv2.resize(img,(width,height))\n",
    "    \"\"\"\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        [tfrecord_file], shuffle=True)\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    image_features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'image/encoded': tf.FixedLenFeature([], tf.string),\n",
    "            'image/label': tf.FixedLenFeature([], tf.int64)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    image = tf.decode_raw(image_features['image/encoded'], tf.uint8)\n",
    "    \n",
    "    # 进行reshape 操作\n",
    "    image = tf.reshape(image, [image_size, image_size, 3])\n",
    "    # 缩放到0-1的区间\n",
    "    image = (tf.cast(image, tf.float32) - 127.5) / 128\n",
    "\n",
    "    label = tf.cast(image_features['image/label'], tf.float32)\n",
    "    image, label = tf.train.batch(\n",
    "        [image, label],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=2,\n",
    "        capacity=1 * batch_size\n",
    "    )\n",
    "    label = tf.reshape(label, [batch_size])\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('image_batch : ', <tf.Tensor 'batch_1:0' shape=(8, 28, 28, 3) dtype=float32>)\n",
      "('label_batch : ', <tf.Tensor 'Reshape_3:0' shape=(8,) dtype=float32>)\n",
      "('label_batch_array:', array([ 5.,  4.,  4.,  1.,  1.,  0.,  3.,  5.], dtype=float32))\n",
      "('image is :', (28, 28, 3), array([[[-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        ..., \n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375]],\n",
      "\n",
      "       [[-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        ..., \n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375]],\n",
      "\n",
      "       [[-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        ..., \n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375]],\n",
      "\n",
      "       ..., \n",
      "       [[-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        ..., \n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375]],\n",
      "\n",
      "       [[-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        ..., \n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375]],\n",
      "\n",
      "       [[-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        ..., \n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375],\n",
      "        [-0.99609375, -0.99609375, -0.99609375]]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "batch_size=8\n",
    "\n",
    "image_size=28\n",
    "\n",
    "# tf_Record路径\n",
    "dataset_dir = os.path.join(\"dataset\", 'train_data.tfrecord_shuffle')\n",
    "\n",
    "# 读取出来的格式是tf的格式\n",
    "image_batch, label_batch= read_single_tfrecord(dataset_dir,batch_size, image_size)\n",
    "\n",
    "print(\"image_batch : \" , image_batch)\n",
    "print(\"label_batch : \" , label_batch)\n",
    "\n",
    "\n",
    "# 直接开启session\n",
    "sess=tf.Session()\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "# 矩阵形式\n",
    "image_batch_array,label_batch_array = sess.run([image_batch,label_batch])\n",
    "\n",
    "print(\"label_batch_array:\",label_batch_array)\n",
    "\n",
    "print(\"image is :\",image_batch_array[0].shape , image_batch_array[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 进行本地训练\n",
    "- 相关依赖导入\n",
    "- 模型定义\n",
    "- 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_convolution(x_image,keep_prob):\n",
    "    \"\"\"\n",
    "    模型定义:\n",
    "        x_images : 输入数据  [batch_size,image_size,image_size,3] \n",
    "    \"\"\"\n",
    "    \n",
    "    # 定义卷积\n",
    "    def conv2d(x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    # 定义池化层\n",
    "    def max_pool_2x2(x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # 定义权重\n",
    "    def weight_variable(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)    \n",
    "    \n",
    "    # 定义偏置\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    \n",
    "    # 5*5核心大小 通道数为3 输出通道数为32\n",
    "    W_conv1 = weight_variable([5, 5, 3, 32])\n",
    "    b_conv1 = bias_variable([32]) \n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    \n",
    "\n",
    "    # 5*5 核心大小, 32个通道数，和上一层匹配，输出64个通道  输出 7*7*64     64 7*7\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "    \n",
    "    # 链接全连接层  7*7*64个点  我们直接用1024\n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])  # 加入一个神经元数目为1024的全连接层\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    # Dropout机制 0.7 \n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "    \n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "    \n",
    "    # 进行概率映射\n",
    "    logits = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    return logits, [W_conv1, b_conv1, W_conv2, b_conv2, W_fc1, b_fc1, W_fc2, b_fc2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(data_dir,max_step,batch_size,image_size=28):\n",
    "    # model\n",
    "    with tf.variable_scope(\"classify_convolution\"):\n",
    "        \"\"\"\n",
    "        定义模型输入的输出:\n",
    "            输入\n",
    "                x: [None 28 28 3]\n",
    "                keep_prob:0.7\n",
    "                \n",
    "            输出:\n",
    "                y:概率映射   0.1 0.8 0.0.  。。。。。。\n",
    "            \n",
    "        \"\"\"\n",
    "        input_images = tf.placeholder(tf.float32, [None, 28,28,3])  # 将某些特殊的操作指定为 \"feed\" 操作, 标记的方法是使用 tf.placeholder() 为这些操作创建占位符.\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        logits, variables = classify_convolution(input_images, keep_prob)\n",
    "\n",
    "\n",
    "    # 这个就是label输入\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    \n",
    "    # 计算网络输出和label的差距，并使用优化器最小化差距，Adam\n",
    "    loss = -tf.reduce_sum(labels * tf.log(logits))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    \n",
    "    \n",
    "    # 计算accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # 进行参数初始化\n",
    "    saver = tf.train.Saver(variables)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    label_batch_array=None\n",
    "    image_batch_array=None\n",
    "    \n",
    "    #  进行数据的读取\n",
    "    image_batch, label_batch= read_single_tfrecord(data_dir, batch_size, image_size)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # 初始化操作\n",
    "        sess.run(init)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        for i in range(max_step):\n",
    "            \n",
    "            # 进行数据和标签转换\n",
    "            image_batch_array,label_batch_one = sess.run([image_batch,label_batch])\n",
    "            label_batch_one=np.array(label_batch_one,np.int16)\n",
    "            label_batch_array=np.zeros((batch_size,10))\n",
    "            label_batch_array[np.arange(batch_size),label_batch_one]=1\n",
    "            \n",
    "            \n",
    "            # 每100次进行测试一次\n",
    "            if i % 100 == 0:\n",
    "                train_accuracy = accuracy.eval(feed_dict={input_images: image_batch_array, labels: label_batch_array, keep_prob: 0.7})\n",
    "                \n",
    "                print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "                \n",
    "            # 开启训练    \n",
    "            sess.run(train_step, feed_dict={input_images: image_batch_array, labels: label_batch_array, keep_prob: 0.7})\n",
    "            \n",
    "        path = saver.save(sess, os.path.join(os.path.dirname(__file__), \"model/convolutional.ckpt\"))\n",
    "        print(\"Saved:\", path)\n",
    "        \n",
    "def test__model(model_path,test_images):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-86a882e25dbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_data.tfrecord_shuffle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "max_step=50000 #最大训练次数\n",
    "batch_size=32  #设置batch_size\n",
    "image_size=28\n",
    "\n",
    "train_model(os.path.join(\"dataset\", 'train_data.tfrecord_shuffle'),50000,32,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TaaS模型训练与测试\n",
    "- \n",
    "\n",
    "##### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_convolution(x_image,keep_prob):\n",
    "    \"\"\"\n",
    "    模型定义:\n",
    "        x_images : 输入数据  [batch_size,image_size,image_size,3] \n",
    "    \"\"\"\n",
    "    \n",
    "    # 定义卷积\n",
    "    def conv2d(x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    # 定义池化层\n",
    "    def max_pool_2x2(x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # 定义权重\n",
    "    def weight_variable(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)    \n",
    "    \n",
    "    # 定义偏置\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    \n",
    "    # 5*5核心大小 通道数为3 输出通道数为32\n",
    "    W_conv1 = weight_variable([5, 5, 3, 32])\n",
    "    b_conv1 = bias_variable([32]) \n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    \n",
    "\n",
    "    # 5*5 核心大小, 32个通道数，和上一层匹配，输出64个通道  输出 7*7*64     64 7*7\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "    \n",
    "    # 链接全连接层  7*7*64个点  我们直接用1024\n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])  # 加入一个神经元数目为1024的全连接层\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    # Dropout机制 0.7 \n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "    \n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "    \n",
    "    # 进行概率映射\n",
    "    logits =tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "    return logits, [W_conv1, b_conv1, W_conv2, b_conv2, W_fc1, b_fc1, W_fc2, b_fc2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TaaS平台训练模板\n",
    "# coding=utf-8\n",
    "import tensorflow as tf\n",
    "from caicloud.clever.tensorflow import dist_base\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "import tensorflow.contrib.slim as slim\n",
    "import datasets_factory\n",
    "\n",
    "\n",
    "# 定义 Tensorflow Flags, 类似于 python argparse.\n",
    "tf.app.flags.DEFINE_boolean('random_flip_up_down', False, \"Whether to random flip up down\")\n",
    "tf.app.flags.DEFINE_boolean('random_brightness', True, \"whether to adjust brightness\")\n",
    "tf.app.flags.DEFINE_boolean('random_contrast', True, \"whether to random constrast\")\n",
    "\n",
    "tf.app.flags.DEFINE_integer('charset_size', 3755, \"Choose the first `charset_size` characters only.\")\n",
    "tf.app.flags.DEFINE_integer('image_size', 28, \"Needs to provide same value as in training.\")\n",
    "tf.app.flags.DEFINE_boolean('gray', True, \"whether to change the rbg to gray\")\n",
    "tf.app.flags.DEFINE_integer('max_steps', 60000, 'the max training steps ')\n",
    "tf.app.flags.DEFINE_integer('eval_steps', 50, \"the step num to eval\")\n",
    "tf.app.flags.DEFINE_integer('save_steps', 500, \"the steps to save\")\n",
    "\n",
    "tf.app.flags.DEFINE_string('checkpoint_dir', 'checkpoint/', 'the checkpoint dir')\n",
    "tf.app.flags.DEFINE_string('train_data_dir', 'dataset/', 'the train dataset dir')\n",
    "tf.app.flags.DEFINE_string('test_data_dir', 'dataset/', 'the test dataset dir')\n",
    "tf.app.flags.DEFINE_string('log_dir', 'ocr_log', 'the logging dir')\n",
    "\n",
    "tf.app.flags.DEFINE_boolean('restore', False, 'whether to restore from checkpoint')\n",
    "tf.app.flags.DEFINE_boolean('epoch', 1, 'Number of epochs')\n",
    "tf.app.flags.DEFINE_integer('batch_size', 64, 'Batch size')\n",
    "tf.app.flags.DEFINE_string('mode', 'train', 'Running mode. One of {\"train\", \"valid\", \"test\"}')\n",
    "tf.app.flags.DEFINE_string('test_image', 'data/test.png', 'image path to test the model')\n",
    "tf.app.flags.DEFINE_integer('num_readers', 4, 'The number of parallel readers that read data from the dataset.')\n",
    "\n",
    "\n",
    "# 通过这句来加载 Flags\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "\n",
    "# 定义一些全局变量，在函数之间调用\n",
    "keep_prob = None   # dropout 开启神经元概率\n",
    "train_op = None   # slim训练操作变量\n",
    "accuracy = None   # 准确率\n",
    "global_step = None   # 全局步数\n",
    "\n",
    "predicted_val_top_k = None   # top k 的预测概率\n",
    "predicted_index_top_k = None   # top k 的预测index\n",
    "merged_summary_op = None   # tf summary 合并记录\n",
    "accuracy_in_top_k = None   # top k 准确率\n",
    "\n",
    "\n",
    "# 定义 Tensorflow 模型函数\n",
    "def model_fn(sync, num_replicas):\n",
    "    # 从全局读取变量\n",
    "    global keep_prob,  train_op, accuracy, global_step\n",
    "\n",
    "    # 创建slim全局步数变量\n",
    "    global_step = slim.create_global_step()\n",
    "\n",
    "    # 读取 TFRecord 文件，获取训练数据队列\n",
    "    dataset = datasets_factory.get_split(\"main_train\",FLAGS.train_data_dir)\n",
    "    provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset,\n",
    "        num_readers=FLAGS.num_readers,\n",
    "        common_queue_capacity=20 * FLAGS.batch_size,\n",
    "        common_queue_min=10 * FLAGS.batch_size)\n",
    "    [image, label] = provider.get(['image', 'label'])\n",
    "\n",
    "    # 图像增强\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.3)\n",
    "    image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "    new_size = tf.constant([FLAGS.image_size, FLAGS.image_size], dtype=tf.int32)\n",
    "    image = tf.image.resize_images(image, new_size)\n",
    "\n",
    "    # 调用 tf.train.batch 来获取 batch 大小的数据流\n",
    "    images, labels = tf.train.batch(\n",
    "        [image, label],\n",
    "        batch_size=FLAGS.batch_size,\n",
    "        num_threads=4,\n",
    "        capacity=5 * FLAGS.batch_size\n",
    "    )\n",
    "\n",
    "    # 定义 tensorflow 占位符\n",
    "    keep_prob = tf.placeholder(dtype=tf.float32, shape=[], name='keep_prob')\n",
    "\n",
    "    \n",
    "        \n",
    "    logits,_=classify_convolution(images,keep_prob)\n",
    "\n",
    "    # 定义loss\n",
    "    \n",
    "    \n",
    "    loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "    \n",
    "    # 定义Update ops\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    if update_ops:\n",
    "        updates = tf.group(*update_ops)\n",
    "        loss = control_flow_ops.with_dependencies([updates], loss)\n",
    "\n",
    "    # 定义优化器，也就是反向传播参数更新过程，使用了AdamOptimizer，可以修改成别的。\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "    # 是否同步更新参数，分布式系统使用\n",
    "    if sync:\n",
    "        num_workers = num_replicas\n",
    "        optimizer = tf.train.SyncReplicasOptimizer(\n",
    "            optimizer,\n",
    "            replicas_to_aggregate=num_workers,\n",
    "            total_num_replicas=num_workers,\n",
    "            name=\"ocr_sync_replicas\")\n",
    "\n",
    "    # 创建 slim 训练变量\n",
    "    train_op = slim.learning.create_train_op(loss, optimizer, global_step=global_step)\n",
    "\n",
    "    # 定义准确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), labels), tf.float32))\n",
    "\n",
    "    # 返回 dist_base\n",
    "    return dist_base.ModelFnHandler(\n",
    "        global_step=global_step,\n",
    "        optimizer=optimizer)\n",
    "\n",
    "\n",
    "local_step = 0\n",
    "\n",
    "\n",
    "# 定义模型训练函数\n",
    "def train_fn(session, num_global_step):\n",
    "    global keep_prob,  train_op, accuracy, local_step, global_step\n",
    "\n",
    "    # validation 测试\n",
    "    if local_step % FLAGS.eval_steps == 1:\n",
    "        val_dict = {keep_prob: 1.0}\n",
    "\n",
    "        validate_acc = session.run(\n",
    "            accuracy,\n",
    "            feed_dict=val_dict)\n",
    "        print(\"After %d training step(s), validation accuracy is %g \" % (\n",
    "            num_global_step, validate_acc))\n",
    "\n",
    "    local_step += 1\n",
    "\n",
    "    train_dict = {keep_prob: 0.8,}\n",
    "\n",
    "    # 训练步\n",
    "    total_loss, cur_global_step = session.run([train_op, global_step], feed_dict=train_dict)\n",
    "\n",
    "    # 每20步打印一次结果\n",
    "    if local_step % 20 == 0:\n",
    "        print('local step: {0}, global step {1}: '\n",
    "              'loss = {2:.4f}'.format(local_step, cur_global_step, total_loss))\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "# 定义训练结束后的评测函数\n",
    "def after_train_hook(sess):\n",
    "    global  keep_prob, accuracy\n",
    "\n",
    "    test_feed = {keep_prob: 1.0}\n",
    "\n",
    "    test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "    print(\"Test accuracy is %g\" % test_acc)\n",
    "\n",
    "\n",
    "# 主函数\n",
    "if __name__ == '__main__':\n",
    "    distTfRunner = dist_base.DistTensorflowRunner(\n",
    "        model_fn=model_fn,\n",
    "        after_train_hook=after_train_hook)\n",
    "    distTfRunner.run(train_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}